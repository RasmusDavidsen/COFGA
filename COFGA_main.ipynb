{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COFGA \n",
    "\n",
    "### Created by:\n",
    "#### Rasmus Davidsen & Lukkas Hamann\n",
    "\n",
    "\n",
    "This notebook takes the resized and one hot encoded training set as input and splits this set into a training set and a validation set. The validation set is 30 percent of the original training set and i randomly selected. \n",
    "\n",
    "The network is constructed and trained, and for each epoch the validation and train mAp scores and loss function are calculated and save as a csv file. \n",
    "\n",
    "Using cuda the notebook has the possibility to utilize a GPU to speed up the training. This was in the project performed by using the DTU HPC cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# custom libraries\n",
    "from mAP import mAP_score\n",
    "from COFGA_dataset import CofgaDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define name of output files\n",
    "results_name = \"results_map.csv\"\n",
    "results_name_AP_val = \"Results_AP_val.csv\"\n",
    "results_name_AP_train = \"Results_AP_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random rotation within +/- 5 degrees\n",
    "degrees = 5\n",
    "rotation_transform = transforms.Compose([transforms.RandomRotation(degrees), transforms.ToTensor(),])\n",
    "\n",
    "# perfoming horizontal flip with a given probability\n",
    "prob = 0.8\n",
    "hoz_transform = transforms.Compose([transforms.RandomHorizontalFlip(prob), transforms.ToTensor(),])\n",
    "\n",
    "#performing vertical flip with a gven probability\n",
    "prob = 0.8\n",
    "vert_transform = transforms.Compose([transforms.RandomVerticalFlip(prob), transforms.ToTensor(),])\n",
    "\n",
    "# color changes\n",
    "brightness = 10\n",
    "contrast = 10\n",
    "saturation = 10\n",
    "hue = 0.25\n",
    "color_transform = transforms.Compose([transforms.ColorJitter(brightness=brightness, contrast=contrast,\n",
    "                                         saturation=saturation, hue=hue), transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "# allows to chose randomly from the different transformations\n",
    "transform_list = transforms.RandomChoice([rotation_transform, hoz_transform,\n",
    "                                          vert_transform, color_transform])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  11617\n"
     ]
    }
   ],
   "source": [
    "# loading the custom dataset\n",
    "dataset = CofgaDataset(csv_file='dataset/train_preprocessed.csv',\n",
    "                             root_dir='dataset/root/train/resized/',\n",
    "                            transform = transform_list)\n",
    "\n",
    "\n",
    "print(\"Total number of images: \",len(dataset))\n",
    "\n",
    "COFGA_headers = pd.read_csv('dataset/train_preprocessed.csv')\n",
    "\n",
    "COFGA_labels = COFGA_headers.columns.tolist()\n",
    "COFGA_labels.pop(0)\n",
    "\n",
    "COFGA_labels.insert(0, \"epoch\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing trainLoader and validation loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images:  8132\n",
      "Number of validation images:  3485\n",
      "\n",
      "Dataloader completed\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# fraction of dataset to be validation set\n",
    "validation_split = 0.3\n",
    "\n",
    "# shuffle dataset when taking the validation set\n",
    "shuffle_dataset = True\n",
    "\n",
    "# seed\n",
    "random_seed = 1\n",
    "\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=False,\n",
    "                                           num_workers=1, sampler=train_sampler, pin_memory=True)\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=False,\n",
    "                                                num_workers=1, sampler=validation_sampler, pin_memory=True)\n",
    "\n",
    "\n",
    "print(\"Number of train images: \", len(train_sampler))\n",
    "print(\"Number of validation images: \", len(validation_sampler))\n",
    "print(\"\\nDataloader completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including cuda for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 37\n",
    "\n",
    "class COFGA_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(COFGA_NET, self).__init__()\n",
    "    \n",
    "        # defining the pretrained network (pretrained on ImageNet data set) \n",
    "        self.model = models.resnet50(pretrained = True)\n",
    "        num_final_in = self.model.fc.in_features\n",
    "        \n",
    "        # changin the last FC layer to fit the outputs for this data\n",
    "        self.model.fc = nn.Linear(num_final_in, 1024)\n",
    "        \n",
    "        # all layers in ResNet are fine tuned\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        \n",
    "        self.bn_1 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.bn_2 = nn.BatchNorm1d(500)\n",
    "        \n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        self.dense_1 = nn.Linear(1024, 500)\n",
    "        \n",
    "        self.l_out = nn.Linear(500, NUM_CLASSES)   \n",
    "        \n",
    "       \n",
    "        \n",
    "    # The forward function defines the flow of the input data and thus decides which layer/chunk goes on top of what.\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        \n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "        x = self.bn_1(x)\n",
    "        \n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.dense_1(x)\n",
    "        \n",
    "        x = self.bn_2(x)\n",
    "        \n",
    "        x = self.relu1(x)\n",
    "              \n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.l_out(x)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "net = COFGA_NET()\n",
    "if use_cuda:\n",
    "    print(\"USING CUDA!\")\n",
    "    net.cuda()\n",
    "    \n",
    "#print(net)\n",
    "\n",
    "print(\"Network constructed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# adddin the Binary cross entropy loss function\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "Now, the network is trained and for each epoch results are saved to a csv file for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# number of epochs to train\n",
    "num_epoch = 200\n",
    "\n",
    "# train list for MAP_score \n",
    "train_MAP_list = [] # Total\n",
    "train_AP_list = [] # Pr. class\n",
    "\n",
    "# validation list for MAP_score \n",
    "val_MAP_list = [] # Total\n",
    "val_AP_list = [] # Pr. class\n",
    "\n",
    "# loss list\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# list for epochs\n",
    "epoch_list = []\n",
    "\n",
    "#training the network\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # allocating memory for train\n",
    "    train_predicted = np.zeros(NUM_CLASSES)\n",
    "    train_target = np.zeros(NUM_CLASSES)\n",
    "    \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        \n",
    "        # get inputs (by names in the custom dataset class)\n",
    "        inputs = data['image']\n",
    "        labels = data['labels']\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = get_variable(Variable(inputs.type(torch.FloatTensor))), get_variable(Variable(labels.type(torch.FloatTensor)))\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # computing the outputs\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        \n",
    "        # outputs to numpy\n",
    "        train_predicted = np.vstack((train_predicted,get_numpy(outputs)))\n",
    "        \n",
    "        \n",
    "        # labels to numpy\n",
    "        train_target = np.vstack((train_target,get_numpy(labels)))\n",
    "\n",
    "        # computing the loss function\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backpropagating\n",
    "        loss.backward()\n",
    "\n",
    "        # updating weights and biases in the network\n",
    "        optimizer.step()\n",
    "        \n",
    "        # appending train loss (summed over the minibatch )\n",
    "        train_loss += get_numpy(loss.data[0])\n",
    "    \n",
    "    \n",
    "    # saving loss, MAP and AP for every 10 epoch  \n",
    "    if int(epoch+1) % int(1) == 0 or epoch==num_epoch or epoch == 0:\n",
    "        \n",
    "        # removing first zero dummy row\n",
    "        train_predicted = train_predicted[1:,:]\n",
    "        train_target = train_target[1:,:]\n",
    "        \n",
    "        \n",
    "        # evaluating network on validation set\n",
    "        net.eval()\n",
    "        # appending epoch to list for result outputs\n",
    "        epoch_list.append(epoch+1)\n",
    "        \n",
    "        # appending train loss to list\n",
    "        train_loss = train_loss/len(train_sampler)\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        # allocating memory for validation MAP\n",
    "        #val_predicted = np.zeros(( int(len(validation_sampler)), NUM_CLASSES))\n",
    "        #val_target = np.zeros(( int(len(validation_sampler)), NUM_CLASSES))\n",
    "        \n",
    "        val_predicted = np.zeros(NUM_CLASSES)\n",
    "        val_target = np.zeros(NUM_CLASSES)\n",
    "        \n",
    "        for i, data in enumerate(validation_loader,0):\n",
    "            # get inputs (by names in the custom dataset class)\n",
    "            inputs = data['image']\n",
    "            labels = data['labels']\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = get_variable(Variable(inputs.type(torch.FloatTensor))), get_variable(Variable(labels.type(torch.FloatTensor)))\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            # computing the loss function\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += get_numpy(loss.data[0])\n",
    "            \n",
    "             \n",
    "             # outputs to numpy\n",
    "            val_predicted = np.vstack((val_predicted,get_numpy(outputs)))\n",
    "        \n",
    "        \n",
    "            # labels to numpy\n",
    "            val_target = np.vstack((val_target,get_numpy(labels)))\n",
    "            \n",
    "            \n",
    "            # outputs to numpy\n",
    "            outputs = get_numpy(outputs)\n",
    "\n",
    "        # removing first zero dummy row\n",
    "        val_predicted = val_predicted[1:,:]\n",
    "        val_target = val_target[1:,:]\n",
    "        \n",
    "        # appending validation loss to list\n",
    "        val_loss = val_loss/len(validation_sampler)                       \n",
    "                               \n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        # calculating MAP\n",
    "        val_MAP = mAP_score(val_predicted, val_target)\n",
    "        train_MAP = mAP_score(train_predicted, train_target)\n",
    "\n",
    "        # appending MAP score to MAP list\n",
    "        val_MAP_list.append(val_MAP[0])\n",
    "        train_MAP_list.append(train_MAP[0])\n",
    "        \n",
    "        # appending AP score to AP list for validation\n",
    "        temp_val = val_MAP[1].tolist()\n",
    "        temp_val.insert(0, epoch+1)\n",
    "           \n",
    "        val_AP_list.append(temp_val)\n",
    "        \n",
    "        # appending AP score to AP list for train\n",
    "        temp_train = train_MAP[1].tolist()\n",
    "        temp_train.insert(0, epoch+1)\n",
    "           \n",
    "        train_AP_list.append(temp_train)\n",
    "        \n",
    "        # printing loss train, validation loss, MAP\n",
    "        print(\"[%d], Train loss: %.3f, Val loss: %.3f, Train mAP: %.3f\"%\n",
    "              (epoch+1, train_loss, val_loss, train_MAP[0]*100), \"%,\",\"Val mAP: %.3f\"%(val_MAP[0]*100), \"%\")\n",
    "    \n",
    "    \n",
    "print('\\nFinished Training \\n')\n",
    "\n",
    "# saving results as pandas dataframe\n",
    "# print('Results validation set\\n')\n",
    "results = pd.DataFrame({'Epoch':epoch_list,'Train loss': train_loss_list, 'Val loss': val_loss_list, 'Train_mAP': train_MAP_list, 'Val_mAP': val_MAP_list})\n",
    "results.to_csv(results_name, sep=\";\", index=False)\n",
    "\n",
    "# Saving AP for validation\n",
    "with open(results_name_AP_val, \"w\") as val_AP_tocsv:\n",
    "    writer = csv.writer(val_AP_tocsv, delimiter=\",\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\")\n",
    "    writer.writerow(COFGA_labels)\n",
    "    for item in val_AP_list:\n",
    "        writer.writerow(item[i] for i in range(0,len(COFGA_labels)))\n",
    "        \n",
    "# Saving AP for train\n",
    "with open(results_name_AP_train, \"w\") as train_AP_tocsv:\n",
    "    writer = csv.writer(train_AP_tocsv, delimiter=\",\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\")\n",
    "    writer.writerow(COFGA_labels)\n",
    "    for item in train_AP_list:\n",
    "        writer.writerow(item[i] for i in range(0,len(COFGA_labels)))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
